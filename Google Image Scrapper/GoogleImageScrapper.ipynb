{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53257d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen as uReq\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "import urllib\n",
    "import time\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92c85a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_to_end(wd):\n",
    "    wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d1eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image_urls(query: str, max_links_to_fetch: int, wd: webdriver, sleep_between_interactions: int = 1):\n",
    "\n",
    "    try:\n",
    "        # build the flipkart query\n",
    "        search_url = \"https://www.google.com/search?tbm=isch&q={0}\"\n",
    "\n",
    "        image_urls = set()\n",
    "        image_count = 0\n",
    "        results_start = 0\n",
    "        google_cookies_accepted = False\n",
    "        search_url = search_url.format(query).replace (\" \",\"%20\")\n",
    "        page_source = wd.page_source\n",
    "\n",
    "        # Accept googlecookies button if not already accepted\n",
    "        if not google_cookies_accepted:\n",
    "            wd = accept_google_cookies(wd, search_url)\n",
    "\n",
    "        while image_count < max_links_to_fetch-1:\n",
    "            scroll_to_end(wd)\n",
    "\n",
    "            #Click thumbnails ang get main image of better resolution\n",
    "            for thumbnail in wd.find_elements(By.CLASS_NAME, \"rg_i\"):\n",
    "                thumbnail.click();\n",
    "                search_page_html = bs(wd.page_source)\n",
    "                thumbnail_results = search_page_html.findAll(\"img\", {\"class\": \"n3VNCb\"})\n",
    "                image_urls = image_urls.union(extract_image_urls(thumbnail_results))\n",
    "                if len(image_urls) >= max_links_to_fetch:\n",
    "                    break\n",
    "                    \n",
    "            image_count = len(image_urls)\n",
    "            # Extract more images to meet the count\n",
    "            if image_count >= max_links_to_fetch:\n",
    "                print(f\"Found: {image_count} image links, done!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Found:\", image_count, \"image links, looking for more ...\")\n",
    "\n",
    "                # Check if more search result pages available\n",
    "                load_more_button = wd.find_element(By.CLASS_NAME, \"mye4qd\")\n",
    "                if load_more_button:\n",
    "                    wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR while fetching image URLs - {e}\")\n",
    "\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34173af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accept_google_cookies(wd: webdriver, url: str):\n",
    "    try:\n",
    "        wd.get(url)\n",
    "        # Find if there is accept all button, if so click it\n",
    "        wd.find_element(By.XPATH, \"//*[@id='yDmH0d']/c-wiz/div/div/div/div[2]/div[1]/div[3]/div[1]/div[1]/form[2]/div/div/button\").click();\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR while accepting the google cookies: - {url} - {e}\")\n",
    "        \n",
    "    return wd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fe01ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_urls(thumbnail_results: list):\n",
    "    image_urls = set()\n",
    "    try:\n",
    "        for img in thumbnail_results:\n",
    "            # extract image urls\n",
    "            if img['src'] and 'http' in img['src']:\n",
    "                image_urls.add(img['src'])\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Extracting the image URLs - {e}\")\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bfa730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_beautifulsoup_object(url: str):\n",
    "    try:\n",
    "        # get all image thumbnail results\n",
    "        uClient = uReq(url)\n",
    "        flipkartPage = uClient.read()\n",
    "        uClient.close()\n",
    "        search_page_html = bs(flipkartPage, \"html.parser\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - building the Beautiful Soup object {url} - {e}\")\n",
    "        \n",
    "    return search_page_html                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a0680a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(folder_path:str,url:str, counter):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "\n",
    "    try:\n",
    "        f = open(os.path.join(folder_path, 'jpg' + \"_\" + str(counter) + \".jpg\"), 'wb')\n",
    "        f.write(image_content)\n",
    "        f.close()\n",
    "        print(f\"SUCCESS - saved {url} - as {folder_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c35465",
   "metadata": {},
   "source": [
    "## All the uploaded images in MongoDB should be found at:\n",
    "https://cloud.mongodb.com/v2/6327733aefeac67e835b73e9#metrics/replicaSet/636ab3dd12263c2b14c32cca/explorer/GoogleImageLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c56dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_image_to_mongodb(folder_path:str, search_term:str, url:str, counter, db):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not uploaded image to mongoDB {url} - {e}\")\n",
    "\n",
    "    try:\n",
    "        dict = {\n",
    "            \"url\": url,\n",
    "            \"image_content\": image_content,\n",
    "            \"local_path\": folder_path + '/jpg' + \"_\" + str(counter) + \".jpg\",\n",
    "        }\n",
    "        collection = db[search_term]\n",
    "        collection.insert_one(dict)\n",
    "        print(f\"SUCCESS - uploaded image to mongoDB {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not uploaded image to mongoDB {url} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d58600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_download(search_term: str, driver_path: str, db, target_path='./images', number_images=10):\n",
    "    try:\n",
    "        target_folder = os.path.join(target_path, '_'.join(search_term.lower().split(' ')))\n",
    "\n",
    "        if not os.path.exists(target_folder):\n",
    "            os.makedirs(target_folder)\n",
    "\n",
    "        with webdriver.Chrome(executable_path=driver_path) as wd:\n",
    "            res = fetch_image_urls(search_term, number_images, wd, sleep_between_interactions=0.5)\n",
    "\n",
    "        counter = 0\n",
    "        for elem in res:\n",
    "            #download image to local folder\n",
    "            download_image(target_folder, elem, counter)\n",
    "            #upload image to MongoDB\n",
    "            upload_image_to_mongodb(target_folder, search_term, elem, counter, db)\n",
    "            counter += 1\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - while search and download images - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b7f4c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DRIVER_PATH = r'chromedriver.exe'\n",
    "# MongoDB cluster and Database\n",
    "cluster = pymongo.MongoClient(\"mongodb+srv://test:test@googleimagelibrary.iyqe6iw.mongodb.net/?retryWrites=true&w=majority\")\n",
    "db = cluster[\"GoogleImageLibrary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e926a",
   "metadata": {},
   "source": [
    "### Update the search_term below to search for Google images with new keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b41c674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for search term is available in DB, So we dont need to scrape it.\n"
     ]
    }
   ],
   "source": [
    "search_term = 'Harry potter'\n",
    "\n",
    "# First check if the data is already available in MongoDB, if not available only then scrape it \n",
    "if(search_term not in db.list_collection_names()):\n",
    "    print(\"Result for search term not available in DB, So we will need to scrape it.\")\n",
    "    search_and_download(search_term=search_term, db=db, driver_path=DRIVER_PATH, number_images=50)\n",
    "else:\n",
    "    print(\"Result for search term is available in DB, So we dont need to scrape it.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
